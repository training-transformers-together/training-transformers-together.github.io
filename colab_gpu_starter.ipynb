{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "colab_gpu_starter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/borzunov/7f39d16308cd85fc07ec026096746e6c/colab_gpu_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivCbovmkZ6dA"
      },
      "source": [
        "### Colab starter kit\n",
        "_This notebook is a part of ["Training Transformers Together"](training-transformers-together.github.io/) NeurIPS demonstration._\n",
        "\n",
        "To participate in the demo training run, you need to follow these steps:\n",
        "1. Log in on [huggingface.co](https://huggingface.co)\n",
        "   * If you don't have an account or if you want to participate anonymously, you can create a new account at [huggingface.co/join](https://huggingface.co/join),\n",
        "2. Join our organization using this [__invite link__](https://huggingface.co/organizations/training-transformers-together/share/otdYDceuoEIGEVXhrmYilPSnVUrnsrjJNz)\n",
        "3. Create a new User Access token at [huggingface.co/settings/token](https://huggingface.co/settings/token) and copy it to the clipboard\n",
        "4. Run the code below: it will ask you for your token, then setup and begin training. That's it!\n",
        "\n",
        "If everything is fine, you will soon see your contribution on the [__training dashboard__](https://huggingface.co/spaces/training-transformers-together/Dashboard).\n",
        "\n",
        "If you have any questions, technical difficulties or just want to chat about distributed training, you can find us here: [![Discord](https://img.shields.io/static/v1?style=default&label=Discord&logo=discord&message=join)](https://discord.gg/uGugx9zYvN)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6Em9rd8rysw",
        "outputId": "7048f302-d565-4d9b-b789-b2d40ae8ebb7"
      },
      "source": [
        "print(\"Installing dependencies...\", end=\" \")\n",
        "!git clone https://github.com/learning-at-home/dalle-hivemind &> log\n",
        "!cd dalle-hivemind && python3 -m pip install -q -r requirements.txt &> log\n",
        "print(\"done!\")\n",
        "\n",
        "exp_name = \"dalle-v1\"\n",
        "%env HF_ORGANIZATION_NAME=training-transformers-together\n",
        "%env HF_MODEL_NAME={exp_name}\n",
        "%env WANDB_API_KEY=bfcfdc646d9481c372938d5370b4f902f7d7f420\n",
        "%env WANDB_ENTITY=learning-at-home\n",
        "%env WANDB_PROJECT=dalle-hivemind-trainers\n",
        "%cd dalle-hivemind\n",
        "\n",
        "import os\n",
        "from huggingface_auth import authorize_with_huggingface\n",
        "os.environ['HF_USER_ACCESS_TOKEN'] = authorize_with_huggingface().hf_user_access_token\n",
        "\n",
        "!ulimit -n 16384 && python3 run_trainer.py --experiment_prefix {exp_name} --client_mode True \\\n",
        " --initial_peers /ip4/52.232.13.142/tcp/31234/p2p/QmWMNBvt3VkVETzQ68Uk44PokVjwA8sSvEEB3z8WwA5ZAS /ip4/193.106.95.184/tcp/31234/p2p/QmegUJSucRpaUrozQoP8B5ocAXhdnxQDrTdoUu5trtCoWc /ip4/52.232.13.143/tcp/31234/p2p/Qme6k7Ey6qz1sqi3QVzwXbop4UA33NP2sv6BStJqnvbRef \\\n",
        " --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --matchmaking_time 20 --allreduce_timeout 120\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n",
            "env: HF_ORGANIZATION_NAME=training-transformers-together\n",
            "env: HF_MODEL_NAME=dalle-v1\n",
            "env: WANDB_API_KEY=bfcfdc646d9481c372938d5370b4f902f7d7f420\n",
            "env: WANDB_ENTITY=learning-at-home\n",
            "env: WANDB_PROJECT=dalle-hivemind-trainers\n",
            "/content/dalle-hivemind\n",
            "\n",
            "Copy a token from 🤗 Hugging Face settings page at \u001b[1mhttps://huggingface.co/settings/token\u001b[0m and paste it here.\n",
            "\n",
            "💡 \u001b[1mTip:\u001b[0m If you don't already have one, you can create a dedicated user access token.\n",
            "Go to \u001b[1mhttps://huggingface.co/settings/token\u001b[0m, click the \u001b[1mNew token\u001b[0m button, and choose the \u001b[1mread\u001b[0m role.\n",
            "\n",
            "🤗 Hugging Face user access token (characters will be hidden): ··········\n",
            "🚀 You will contribute to the collaborative training under the username borzunov\n",
            "Dec 07 18:46:21.942 [\u001b[1m\u001b[34mINFO\u001b[0m] Trying 3 initial peers: ['/ip4/52.232.13.142/tcp/31234/p2p/QmWMNBvt3VkVETzQ68Uk44PokVjwA8sSvEEB3z8WwA5ZAS', '/ip4/193.106.95.184/tcp/31234/p2p/QmegUJSucRpaUrozQoP8B5ocAXhdnxQDrTdoUu5trtCoWc', '/ip4/52.232.13.143/tcp/31234/p2p/Qme6k7Ey6qz1sqi3QVzwXbop4UA33NP2sv6BStJqnvbRef']\n",
            "Dec 07 18:46:21.943 [\u001b[1m\u001b[34mINFO\u001b[0m] Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Downloading: 100% 773k/773k [00:00<00:00, 4.15MB/s]\n",
            "Downloading: 100% 1.32M/1.32M [00:00<00:00, 6.56MB/s]\n",
            "Downloading: 100% 1.17k/1.17k [00:00<00:00, 965kB/s]\n",
            "Dec 07 18:46:24.198 [\u001b[1m\u001b[34mINFO\u001b[0m] Checkpoint dir outputs, contents []\n",
            "Dec 07 18:46:24.198 [\u001b[1m\u001b[34mINFO\u001b[0m] Creating model\n",
            "Dec 07 18:46:26.099 [\u001b[1m\u001b[34mINFO\u001b[0m] Trainable parameters: 125894244\n",
            "Dec 07 18:46:37.035 [\u001b[1m\u001b[34mINFO\u001b[0m] Access for user borzunov has been granted until 2021-12-08 00:46:37.003837 UTC\n",
            "🚀 You will contribute to the collaborative training under the username borzunov\n",
            "Dec 07 18:46:39.806 [\u001b[1m\u001b[34mINFO\u001b[0m] Created client mode peer with peer_id=QmYALkqZ79fV8bQ5DcNtCLpmvYRYPzxsH6dEtUo55s9GBj\n",
            "Dec 07 18:46:40.291 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 331 samples for epoch #89 from 14 peers. ETA 901.40 sec (refresh in 10.00 sec)\n",
            "Dec 07 18:46:41.392 [\u001b[1m\u001b[34mINFO\u001b[0m] Initializing optimizer manually since it has no tensors in state dict. To override this, provide initialize_optimizer=False\n",
            "Dec 07 18:46:51.425 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 371 samples for epoch #89 from 15 peers. ETA 937.31 sec (refresh in 10.00 sec)\n",
            "Downloading: 100% 2.14k/2.14k [00:00<00:00, 1.69MB/s]\n",
            "Dec 07 18:46:58.744 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mdatasets.builder._create_builder_config:377\u001b[0m] Using custom data configuration default\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1054: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 100000000000000000000\n",
            "  Num Epochs = 9223372036854775807\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100000000000000000000\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mttt-data\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "Dec 07 18:47:02.118 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 411 samples for epoch #89 from 15 peers. ETA 991.72 sec (refresh in 10.00 sec)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mborzunov\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/learning-at-home/dalle-hivemind-trainers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/learning-at-home/dalle-hivemind-trainers/runs/sqp81n3v\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/dalle-hivemind/wandb/run-20211207_184659-sqp81n3v\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "Dec 07 18:47:04.019 [\u001b[1m\u001b[34mINFO\u001b[0m] Loading state from peers\n",
            "Dec 07 18:47:04.427 [\u001b[1m\u001b[34mINFO\u001b[0m] Downloading parameters from peer QmfV68pYRU1EBZZRwGsBxBhSGi3oCeP7h9MJtvP6CdkPba\n",
            "Dec 07 18:47:37.029 [\u001b[1m\u001b[34mINFO\u001b[0m] Finished downloading state from QmfV68pYRU1EBZZRwGsBxBhSGi3oCeP7h9MJtvP6CdkPba\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "Dec 07 18:47:41.455 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 533 samples for epoch #89 from 15 peers. ETA 1043.39 sec (refresh in 10.00 sec)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:705: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  tensor = as_tensor(value)\n",
            "Dec 07 18:47:52.213 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 566 samples for epoch #89 from 15 peers. ETA 1047.79 sec (refresh in 10.00 sec)\n",
            "Dec 07 18:48:00.123 [\u001b[1m\u001b[34mINFO\u001b[0m] Current epoch: 89\n",
            "Dec 07 18:48:00.123 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 1 samples\n",
            "Dec 07 18:48:00.123 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 0.10496910699611124 samples/sec\n",
            "Dec 07 18:48:00.123 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 6.7212\n",
            "Dec 07 18:48:02.860 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 606 samples for epoch #89 from 15 peers. ETA 1012.79 sec (refresh in 10.00 sec)\n",
            "Dec 07 18:48:13.479 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 635 samples for epoch #89 from 15 peers. ETA 1010.28 sec (refresh in 10.00 sec)\n",
            "Dec 07 18:48:24.106 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 677 samples for epoch #89 from 15 peers. ETA 1009.48 sec (refresh in 10.00 sec)\n",
            "Dec 07 18:48:34.657 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 707 samples for epoch #89 from 16 peers. ETA 993.28 sec (refresh in 10.00 sec)\n",
            "Dec 07 18:48:45.306 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 746 samples for epoch #89 from 16 peers. ETA 937.41 sec (refresh in 10.00 sec)\n",
            "Dec 07 18:48:55.950 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 800 samples for epoch #89 from 16 peers. ETA 826.76 sec (refresh in 10.00 sec)\n",
            "Dec 07 18:49:06.556 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 836 samples for epoch #89 from 16 peers. ETA 877.46 sec (refresh in 10.00 sec)\n",
            "Dec 07 18:49:17.189 [\u001b[1m\u001b[34mINFO\u001b[0m] dalle-v1 accumulated 872 samples for epoch #89 from 16 peers. ETA 890.84 sec (refresh in 10.00 sec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vpBPLl0SLHlx"
      }
    }
  ]
}
